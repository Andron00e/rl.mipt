{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Глубокий кросс-энтропийный метод\n",
    "\n",
    "#### дедлайн (жёсткий) задания: 13 марта, 23:59 UTC+3\n",
    "\n",
    "#### при сдаче задания нужно данный файл и сгенерированные в результате его работы файлы и директории поместить в архив, сохраняя относительные пути, и послать архив в систему сдачи\n",
    "\n",
    "В данной работе будет рассмотрено обобщение кросс-энтропийного метода на случай параметризации агента с помощью нейросети. Перед вами будет поставлена задача обучить многослойную нейронную сеть для решения простых игр в непрерывных пространствах состояний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: ФИО, номер группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для корректной работы ноутбука может понадобиться исполнение следующих команд:\n",
    "# (инструкция для UNIX-подобных систем):\n",
    "# !apt-get install -y xvfb x11-utils ffmpeg libav-tools python-opengl swig\n",
    "# !pip install --upgrade piglet pyvirtualdisplay joblib box2d-py gym[all]==0.18.3\n",
    "# (команда для MacOS системы):\n",
    "# !brew install XQuartz swig # для использования xvfb в MacOS сервере (не опробовано)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации среды на headless-сервере с UNIX-подобной ОС (например, Google Colab) раскомментируйте следующую ячейку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvirtualdisplay import Display # для визуализации на UNIX-подобном сервере\n",
    "\n",
    "\n",
    "# virtual_display = Display(visible=0, size=(1920, 1080))\n",
    "# virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве альтернативы предыдущей ячейке можно раскомментировать ячейку ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:08.272254Z",
     "start_time": "2019-09-18T14:49:08.268020Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# XVFB будет запущен в случае исполнения на сервере, если раскомментировать код ниже:\n",
    "# import os\n",
    "# if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "#    !bash ../xvfb start\n",
    "#    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрыть предупреждения: эстетическая опция, можно не выполнять ячейку ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:15.992701Z",
     "start_time": "2019-09-18T14:49:08.275069Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.)\n",
    "\n",
    "# если есть вывод \"<classname> has no attribute .env\", удалите .env или обновите gym\n",
    "env = gym.make(\"CartPole-v1\").env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "print(\"размерность вектора состояний dim =\", state_dim)\n",
    "print(\"n_actions =\", n_actions)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:15.999558Z",
     "start_time": "2019-09-18T14:49:15.995465Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Нейросетевая политика, основная задача (10 баллов)\n",
    "\n",
    "Для текущей задачи будет использована упрощённая нейронная сеть, реализованная в библиотеке __[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)__. Что для решения задачи понадобится:\n",
    "\n",
    "* `agent.partial_fit(states, actions)` - выполнение одного прохода (одной эпохи) по данным для настройки параметров. В ходе вызова данного метода происходит приближённая максимизация вероятности :actions: при условии :states:\n",
    "* `agent.predict_proba(states)` - оценка вероятностей всех действий, матрица формы __[len(states), n_actions]__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:18.770363Z",
     "start_time": "2019-09-18T14:49:16.001843Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "\n",
    "agent = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 20),\n",
    "    activation='tanh',\n",
    ")\n",
    "\n",
    "# инициализация агента на размерности пространства состояний и пространства действий\n",
    "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:18.800907Z",
     "start_time": "2019-09-18T14:49:18.772874Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_session(agent, t_max=1000, test=False):\n",
    "    \"\"\"\n",
    "    Сыграть отдельный эпизод, используя нейросетевую параметризацию агента.\n",
    "    Останов после :t_max: шагов среды.\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # Используйте модель агента для оценки распределения на действия для текущего состояния :s:\n",
    "        probs = <Код для вывода оценки вероятности по выходу нейросети>\n",
    "\n",
    "        assert probs.shape == (n_actions,), \"Нужно получить вектор вероятностей (функция np.reshape в помощь)\"\n",
    "        \n",
    "        # Используйте текущую оценку политики для выбора действия\n",
    "        if test:\n",
    "            # на тестовом прогоне или на валидации используйте\n",
    "            # детерминированную стратегию\n",
    "            a = <Выбор наиболее вероятного действия>\n",
    "            # ^-- подсказка: попробуйте функцию np.argmax\n",
    "        else:\n",
    "            # сэмплирование пропорционально политике $\\pi(a|s)$,\n",
    "            # не нужно выбирать детерминированно наиболее вероятное действие\n",
    "            a = <Сэмплирование действия относительно распределения, задаваемого политикой>\n",
    "            # ^-- подсказка: попробуйте функцию np.random.choice\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # Запись статистики текущего эпизода\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:18.825783Z",
     "start_time": "2019-09-18T14:49:18.804175Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_states, dummy_actions, dummy_reward = generate_session(agent, t_max=5)\n",
    "print(\"states:\", np.stack(dummy_states))\n",
    "print(\"actions:\", dummy_actions)\n",
    "print(\"reward:\", dummy_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Шаги кросс-энтропийного метода\n",
    "Общая схема кросс-энтропийного метода (CEM) приведена на странице 35 (параграф 2.2.4.) учебного [пособия](https://arxiv.org/pdf/2201.09746.pdf).\n",
    "\n",
    "Глубокий CEM использует точно такую же стратегию, что и обычный CEM.\n",
    "\n",
    "Главное отличие состоит в том, что теперь каждое наблюдение не число, а `float32` вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:18.845899Z",
     "start_time": "2019-09-18T14:49:18.827600Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Выбрать states и actions из игр с rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, оба 1D lists of states и actions из наилучших эпизодов\n",
    "\n",
    "    Просьба сохранять порядок elite states и actions \n",
    "    [то есть сортированы по номерам эпизодов и в хронологическом порядке в каждом эпизоде]\n",
    "\n",
    "    Просьба не считать по умолчанию states как целочисленные значения\n",
    "    (они позже примут другой формат).\n",
    "    \"\"\"\n",
    "\n",
    "    <Ваша имплементация>\n",
    "        \n",
    "    # Заметим, что это отличается от табличного случая.\n",
    "    # Теперь наша нейронная сеть обучается на одном объекте с входной размерностью `(1, n_states)`.\n",
    "    # То есть следует использовать `np.squeeze`, чтобы избавиться от ведущего измерения в `select_elites`.\n",
    "    \n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Цикл обучения\n",
    "Генерация эпизодов, выбор N лучших и обучение на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T14:49:18.868655Z",
     "start_time": "2019-09-18T14:49:18.848299Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    Функция-помощник, которая визуализирует процесс обучения.\n",
    "    Никакой крутой математики здесь нет, только построение графиков.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    print(\"средняя награда = %.3f, порог=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Средние награды')\n",
    "    plt.plot(list(zip(*log))[1], label='Пороги наград')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"перцентиль\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:04:30.103876Z",
     "start_time": "2019-09-18T14:49:18.873681Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(font_scale=1.)\n",
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"Итерация метода № {}:\".format(i + 1))\n",
    "    # генерация новых эпизодов\n",
    "    sessions = [<генерация списка из n_sessions эпизодов>]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "\n",
    "    elite_states, elite_actions =  <выбор наилучшых действий с помощью select_elites()>\n",
    "\n",
    "    <partial_fit агента для обучения на elite_actions (y, целевой переменной)\n",
    "     при условии elite_states (X, данных)>\n",
    "\n",
    "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
    "\n",
    "    if np.mean(rewards_batch) > 190:\n",
    "        print(\"Вы выиграли! Можете прервать процедуру обучения с помощью сигнала KeyboardInterrupt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:04:43.484212Z",
     "start_time": "2019-09-18T15:04:30.106224Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# запись эпизодов\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "for kind, directory in [(True, \"test\"), (False, \"sample\")]:\n",
    "    env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"),\n",
    "                               directory=\"videos/CartPole-v1/{}\".format(directory), force=True)\n",
    "    sessions = [generate_session(<Попробуйте поварьировать значение test = {TRUE, FALSE}>) for _ in range(100)]\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детерминированная политика:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:04:43.508057Z",
     "start_time": "2019-09-18T15:04:43.498495Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# демонстрация видео\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/CartPole-v1/test\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/CartPole-v1/test/\"+video_names[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастическая политика:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/CartPole-v1/sample\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/CartPole-v1/sample/\"+video_names[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Дополнительные задания\n",
    "\n",
    "### Глубокий кросс-энтропийный метод\n",
    "\n",
    "На данный момент у Вас должна быть достаточно хорошая награда в [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/), чтобы считать основное задание выполненным (см. ссылку). Время попробовать что-нибудь посложнее.\n",
    "\n",
    "### Задания (вплоть до 5 баллов)\n",
    "\n",
    "* __2.1__ Выбрать следующие среды: MountainCar-v0 и LunarLander-v2.\n",
    "  * Для MountainCar, получить среднее вознаграждение __как минимум -150__\n",
    "  * For LunarLander, получить среднее вознаграждение __как минимум +50__\n",
    "\n",
    "Рекомендуем обратить внимание на раздел с советами ниже, это важно.\n",
    "\n",
    "* __2.2__ Изучить зависимость скорости обучения агента от гипрепараметров алгоритма в среде MountainCar-v0. Постараться продемонстрировать возможность ускорения обучения хотя бы в два раза при грамотном подборе гиперпараметров\n",
    "  * Очевидное улучшение: использовать [joblib](https://joblib.readthedocs.io/en/stable/) или multiprocessing\n",
    "  * Попробовать переиспользовать сэмплы из 3-5 последних итераций при вычислении порога и при обучении\n",
    "  * Поэкспериментировать с количеством итераций обучения и шагом метода обучения (learning rate) нейронной сети (смотреть params)\n",
    "\n",
    "* __При сдаче задания требуется перечислить в данном файле, что было сделано и чего добились__\n",
    "  \n",
    "### Советы\n",
    "* Страница Gym: [MountainCar](https://www.gymlibrary.dev/environments/classic_control/mountain_car/), [LunarLander](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
    "* Эпизоды в MountainCar могут длиться более 10k+ шагов. Убедитесь, что параметр ```t_max``` не меньше 10k.\n",
    " * Также может быть хорошей идеей строго отбирать награды через \">\", а не \">=\" (strictly_select_elites). Если 90% Ваших эпизодов имеют награду -10k и 20% лучше, тода если Вы используйте перцентиль 20% как порог, R >= порог __не может отклонить неуспешные эпизоды__, в то время как R > справляется с этим корректно.\n",
    "* _проблема сред gym_: некоторые версии gym ограничивают эпизод 200 шагами. Это ограничивает возможности CEM в обучении в большинстве случаев. Убедитесь, что Ваш агент способен симулировать эпизоды с заданным __t_max__, и если нет, то попробуйте `env = gym.make(\"MountainCar-v0\").env` или в ином случае избавьтесь от TimeLimit wrapper.\n",
    "* Если Вы пользуетесь старой _swig_ библиотекой для LunarLander-v2, у Вас может возникнуть ошибка. Детали по [ссылке](https://github.com/openai/gym/issues/100) с решением проблемы.\n",
    "* Если CEM не будет обучаться, то построение диаграмм распределения награды и запись видео эпизодов могут помочь: они Вас могут натолкнуть на идею исправления.\n",
    "* 20-нейронной сети может не хватить, не стесняйтесь экспериментировать.\n",
    "\n",
    "Ячейка с кодом ниже может оказаться полезной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mountain_car(env, agent):\n",
    "    xs = np.linspace(env.min_position, env.max_position, 100)\n",
    "    vs = np.linspace(-env.max_speed, env.max_speed, 100)\n",
    "    grid = np.dstack(np.meshgrid(xs, vs)).transpose(1, 0, 2)\n",
    "    grid_flat = grid.reshape(len(xs) * len(vs), 2)\n",
    "    probs = agent.predict_proba(grid_flat).reshape(len(xs), len(vs), 3)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции-помощники для инициализации среды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T12:21:17.631855Z",
     "start_time": "2019-09-18T12:21:17.626438Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_env(name, classification=True):\n",
    "    env = gym.make(name).env\n",
    "    \n",
    "    env.reset()\n",
    "    if classification:\n",
    "        n_actions = env.action_space.n\n",
    "    else:\n",
    "        n_actions = sum(env.action_space.shape)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    \n",
    "    plt.imshow(env.render(\"rgb_array\"))\n",
    "    print(\"state vector dim =\", state_dim)\n",
    "    print(\"n_actions =\", n_actions)\n",
    "    \n",
    "    env.close()\n",
    "    return env, n_actions, state_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строгий отбор states и actions на основе перцентилей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:27:01.282729Z",
     "start_time": "2019-09-18T11:27:01.270077Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def strictly_select_elites(states_batch, actions_batch, rewards_batch, percentile=50, classification=True):\n",
    "    \"\"\"\n",
    "    Выбрать states и actions из игр с rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, оба 1D lists of states и actions из наилучших эпизодов\n",
    "\n",
    "    Просьба сохранять порядок elite states и actions \n",
    "    [то есть сортированы по номерам эпизодов и в хронологическом порядке в каждом эпизоде]\n",
    "\n",
    "    Просьба не считать по умолчанию states как целочисленные значения\n",
    "    (они позже примут другой формат).\n",
    "    Отбор в случае решения задачи классификации и в случае решения задачи регрессии несколько отличаются.\n",
    "    \"\"\"\n",
    "\n",
    "    < Ваша имплементация >\n",
    "    \n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обёртка для инициализации агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:07:35.917489Z",
     "start_time": "2019-09-18T15:07:35.909155Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "def init_agent(env, classification=True, **params):\n",
    "    if classification:\n",
    "        agent = MLPClassifier(\n",
    "            hidden_layer_sizes=(20, 20, 20, 20),\n",
    "            activation='tanh',\n",
    "        )\n",
    "    else:\n",
    "        agent = MLPRegressor(\n",
    "            hidden_layer_sizes=(20, 20, 20, 20),\n",
    "            activation='tanh',\n",
    "        )\n",
    "    \n",
    "    agent.set_params(**params)\n",
    "    \n",
    "    if classification:\n",
    "        n_actions = env.action_space.n\n",
    "        agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))\n",
    "    else:\n",
    "        n_actions = sum(env.action_space.shape)\n",
    "        if n_actions > 1:\n",
    "            agent.partial_fit([env.reset()], np.random.randn(1, n_actions))\n",
    "        else:\n",
    "            agent.partial_fit([env.reset()], np.random.randn(n_actions))\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генератор эпизодов, приспособленный к параллелизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "def generate_parallel_session(agent, name='MountainCar-v0', t_max=10000, classification=True,\n",
    "                              epsilon=0.01, agent_mul_fac=1.0, test=False, env=None):\n",
    "    \"\"\"\n",
    "    Сыграть отдельный эпизод, используя нейросетевую параметризацию агента.\n",
    "    Останов после :t_max: шагов среды.\n",
    "    \"\"\"\n",
    "    \n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "    \n",
    "    if env is None:\n",
    "        env = gym.make(name).env\n",
    "    if classification:\n",
    "        n_actions = env.action_space.n\n",
    "    else:\n",
    "        n_actions = sum(env.action_space.shape)\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        \n",
    "        if classification:\n",
    "            probs = < оценка вероятностей нейросетевой модели >\n",
    "\n",
    "            assert probs.shape == (n_actions,), \"Нужно получить вектор вероятностей\" +\\\n",
    "                \"(функция np.reshape в помощь)\"\n",
    "            \n",
    "            if test:\n",
    "                a = < наиболее вероятное действия относительно probs >\n",
    "            else:\n",
    "                a = < сэмпл действия из распределения probs >\n",
    "        else:\n",
    "            expected_action = < оценка с помощью модели среднего действия, помноженного на agent_mul_fac >\n",
    "            \n",
    "            if test:\n",
    "                a = < само expected_action >\n",
    "            else:\n",
    "                a = < сэмпл из Normal(expected_action, epsilon) >\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    del env, s, new_s, a\n",
    "    \n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для обучения агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:27:05.573082Z",
     "start_time": "2019-09-18T11:27:05.556702Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "# from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def train_agent(classification=True, epsilon=0.01, name='MountainCar-v0', n_train_steps=100,\n",
    "                n_game_steps=10000, n_sessions=100, percentile=70, goal_score=-150, history_length=4,\n",
    "                n_jobs=16, verbose=True, agent_mul_fac=1.0, **params):\n",
    "    env = gym.make(name).env\n",
    "    if classification:\n",
    "        n_actions = env.action_space.n\n",
    "    else:\n",
    "        n_actions = sum(env.action_space.shape)\n",
    "    \n",
    "    agent = init_agent(env, classification, **params)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Шаг агента = {}'.format(agent.learning_rate_init), flush=True)\n",
    "    \n",
    "    log = []\n",
    "    sessions = deque([], history_length * n_sessions)\n",
    "    \n",
    "    for i in range(n_train_steps):\n",
    "        if n_jobs > 1:\n",
    "            # pool = Pool(processes=n_jobs, maxtasksperchild=10)\n",
    "            # sessions.extend(pool.map(partial(generate_parallel_session, agent, name, n_game_steps,\n",
    "            #                                  classification, epsilon, agent_mul_fac), [False] * n_sessions))\n",
    "            # pool.close()\n",
    "            # pool.join()\n",
    "            # del pool\n",
    "            sessions.extend(Parallel(n_jobs=n_jobs)(delayed(partial(generate_parallel_session, agent, name,\n",
    "                                                                    n_game_steps, classification,\n",
    "                                                                    epsilon, agent_mul_fac))(x)\\\n",
    "                                                    for x in [False] * n_sessions))\n",
    "        else:\n",
    "            sessions.extend([generate_parallel_session(\n",
    "                agent, name, n_game_steps, classification, epsilon, agent_mul_fac,\n",
    "                False) for _ in range(n_sessions)])\n",
    "        \n",
    "        states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "        elite_states, elite_actions = strictly_select_elites(\n",
    "            states_batch, actions_batch, rewards_batch, percentile, classification)\n",
    "        \n",
    "        if classification:\n",
    "            agent.partial_fit(elite_states, elite_actions)\n",
    "        else:\n",
    "            elite_actions = elite_actions if (len(elite_actions.shape) < 2) or (elite_actions.shape[1] > 1)\\\n",
    "                            else elite_actions.reshape(-1)\n",
    "            agent.partial_fit(elite_states, elite_actions / agent_mul_fac)\n",
    "        \n",
    "        if verbose:\n",
    "            inter_min = np.min(rewards_batch)\n",
    "            min_lim = -n_game_steps if -n_game_steps < inter_min else inter_min\n",
    "            inter_max = np.max(rewards_batch)\n",
    "            max_lim = goal_score if goal_score > inter_max else inter_max\n",
    "            show_progress(rewards_batch, log, percentile, reward_range=[min_lim, max_lim])\n",
    "            \n",
    "            if np.mean(rewards_batch) > goal_score:\n",
    "                print(\"Вы выиграли! Можете прервать процедуру обучения с помощью сигнала KeyboardInterrupt.\")\n",
    "    \n",
    "    if verbose:\n",
    "        mean_reward = np.mean(rewards_batch)\n",
    "        threshold = np.percentile(rewards_batch, percentile)\n",
    "        print(\"средняя награда = %.3f, порог=%.3f\" % (mean_reward, threshold))\n",
    "        del mean_reward, threshold\n",
    "    \n",
    "    del env, sessions, states_batch, actions_batch, rewards_batch, elite_states, elite_actions\n",
    "    \n",
    "    return agent, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для постановки экспериментов, зависящих от набора гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def training_experiment(n_jobs_list, history_length_list, learning_rate_init_list, n_train_steps_list,\n",
    "                        n_samples=5, name='MountainCar-v0', n_game_steps=10000, n_sessions=100, percentile=70,\n",
    "                        goal_score=-150, **params):\n",
    "    experiment_data = []\n",
    "    \n",
    "    for n_jobs in n_jobs_list:\n",
    "        for history_length in history_length_list:\n",
    "            for learning_rate_init in learning_rate_init_list:\n",
    "                params['learning_rate_init'] = learning_rate_init\n",
    "                for n_train_steps in n_train_steps_list:\n",
    "                    elapsed_time_list = []\n",
    "                    log_list = []\n",
    "                    \n",
    "                    for i in range(n_samples):\n",
    "                        print(\n",
    "                            'Запуск: n_jobs = {}, history_length = {},\\n'.format(\n",
    "                                n_jobs, history_length) +\\\n",
    "                            '                  learning_rate_init = {}, n_train_steps = {};\\n'.format(\n",
    "                                learning_rate_init, n_train_steps) +\\\n",
    "                            'сэмпл {} из {}.'.format(\n",
    "                                i + 1, n_samples), flush=True)\n",
    "                        \n",
    "                        elapsed_time = time()\n",
    "                        \n",
    "                        agent, log = train_agent(\n",
    "                            classification=True, epsilon=0.01, name=name, n_train_steps=n_train_steps,\n",
    "                            n_game_steps=n_game_steps, n_sessions=n_sessions, percentile=percentile,\n",
    "                            goal_score=goal_score, history_length=history_length, n_jobs=n_jobs, verbose=True,\n",
    "                            **params)\n",
    "                        \n",
    "                        elapsed_time_list.append(time() - elapsed_time)\n",
    "                        \n",
    "                        log_list.append(log)\n",
    "                        \n",
    "                        del log, agent\n",
    "                        \n",
    "                    results = {\n",
    "                               'name': name,\n",
    "                               'goal_score': goal_score,\n",
    "                               'n_jobs': n_jobs,\n",
    "                               'elapsed_time_list': elapsed_time_list,\n",
    "                               'history_length': history_length,\n",
    "                               'learning_rate_init': learning_rate_init,\n",
    "                               'n_train_steps': n_train_steps,\n",
    "                               'log_list': log_list\n",
    "                              }\n",
    "                        \n",
    "                    experiment_data.append(results)\n",
    "                    \n",
    "                    del elapsed_time_list, log_list\n",
    "    return experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция-помощник для визуализации эффектов от переиспользования сэмплов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_length_experiment_results(df_data):\n",
    "    history_length_vals = []\n",
    "    train_step_vals = []\n",
    "    reward_vals = []\n",
    "    mean_threshold_vals = []\n",
    "    \n",
    "    for config in df_data:\n",
    "        for log in config['log_list']:\n",
    "            mean_rewards, reward_thresholds = zip(*log)\n",
    "            mean_rewards, reward_thresholds = list(mean_rewards), list(reward_thresholds)\n",
    "            reward_vals.extend(mean_rewards)\n",
    "            reward_vals.extend(reward_thresholds)\n",
    "            train_steps_num = len(mean_rewards)\n",
    "            train_step_vals.extend(range(train_steps_num))\n",
    "            train_step_vals.extend(range(train_steps_num))\n",
    "            history_length_vals.extend([config['history_length']] * train_steps_num * 2)\n",
    "            mean_threshold_vals.extend(['mean'] * train_steps_num)\n",
    "            mean_threshold_vals.extend(['threshold'] * train_steps_num)\n",
    "    \n",
    "    df = pd.DataFrame({'Переиспользованных шагов': history_length_vals, 'Шаг №': train_step_vals,\n",
    "                       'Награда': reward_vals, 'Вид награды': mean_threshold_vals})\n",
    "    last_steps_count = np.sort(df.loc[:, 'Переиспользованных шагов'].unique())\n",
    "    max_n_colors = last_steps_count.size\n",
    "    palette = dict(zip(last_steps_count, sns.hls_palette(max_n_colors, l=.45, s=.8)))\n",
    "    \n",
    "    sns.set(font_scale=1.35)\n",
    "    g = sns.relplot(x='Шаг №', y='Награда', hue='Переиспользованных шагов', style='Вид награды', kind='line',\n",
    "                    data=df, height=8, aspect=1.5, palette=palette)\n",
    "    g.fig.suptitle('Влияние количества переиспользованных шагов при обучении {}'.format(df_data[0]['name']))\n",
    "    clear_output(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция-помощник для визуализации влияния шага обучения и количества итераций при обучении агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lrt_experiment_results(df_data):\n",
    "    learning_rate_vals = []\n",
    "    max_train_steps_vals = []\n",
    "    train_step_vals = []\n",
    "    reward_vals = []\n",
    "    mean_threshold_vals = []\n",
    "    \n",
    "    for config in df_data:\n",
    "        for log in config['log_list']:\n",
    "            mean_rewards, reward_thresholds = zip(*log)\n",
    "            mean_rewards, reward_thresholds = list(mean_rewards), list(reward_thresholds)\n",
    "            reward_vals.extend(mean_rewards)\n",
    "            reward_vals.extend(reward_thresholds)\n",
    "            train_steps_num = len(mean_rewards)\n",
    "            train_step_vals.extend(range(train_steps_num))\n",
    "            train_step_vals.extend(range(train_steps_num))\n",
    "            learning_rate_vals.extend([config['learning_rate_init']] * train_steps_num * 2)\n",
    "            max_train_steps_vals.extend([config['n_train_steps']] * train_steps_num * 2)\n",
    "            mean_threshold_vals.extend(['mean'] * train_steps_num)\n",
    "            mean_threshold_vals.extend(['threshold'] * train_steps_num)\n",
    "                \n",
    "    df = pd.DataFrame({'Шаг метода': learning_rate_vals, 'Максимум шагов': max_train_steps_vals,\n",
    "                       'Шаг №': train_step_vals, 'Награда': reward_vals,\n",
    "                       'Вид награды': mean_threshold_vals})\n",
    "    lr_vals = np.sort(df.loc[:, 'Learning rate'].unique())\n",
    "    max_n_colors = lr_vals.size\n",
    "    palette = dict(zip(lr_vals, sns.hls_palette(max_n_colors, l=.45, s=.8)))\n",
    "    \n",
    "    sns.set(font_scale=1.35)\n",
    "    g = sns.relplot(x='Шаг №', y='Награда', hue='Шаг метода', style='Вид награды', kind='line',\n",
    "                    row='Максимум шагов', data=df, height=6, aspect=1.8, palette=palette)\n",
    "    g.fig.suptitle('Влияние шага метода и количества итераций на {}'.format(df_data[0]['name']),\n",
    "                   x=0.8, y=1.05)\n",
    "    leg = g._legend\n",
    "    for lr, label in zip(lr_vals, leg.texts[1:]):\n",
    "        label.set_text(\"{:.4f}\".format(lr))\n",
    "    clear_output(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для записи сэмплированных игровых эпизодов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_sessions(agent, name, t_max=10000, classification=True, epsilon=0.01, agent_mul_fac=1.0):\n",
    "    for kind, directory in [(True, \"test\"), (False, \"sample\")]:\n",
    "        env = gym.wrappers.Monitor(gym.make(name),\n",
    "                                   directory=\"videos/{}/{}\".format(name, directory), force=True)\n",
    "        sessions = [generate_parallel_session(agent, name=name, t_max=t_max, classification=classification,\n",
    "                                              epsilon=epsilon, agent_mul_fac=agent_mul_fac, test=kind,\n",
    "                                              env=env) for _ in range(100)]\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, позволяющая вставить запись игрового эпизода в ноутбук:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(name, directory):\n",
    "    path = \"./videos/{}/{}\".format(name, directory)\n",
    "    video_names = list(\n",
    "        filter(lambda s: s.endswith(\".mp4\"), os.listdir(path)))\n",
    "    return HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(path + \"/\" + video_names[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сетка параметров для проведения экспериментов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "< n_jobs_list = сетка на количество процессов\n",
    "history_length_list = сетка на размер истории\n",
    "learning_rate_init_list = сетка на шаги метода\n",
    "n_train_steps_list = сетка на количество итераций метода >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глубокий кросс-энтропийный метод в среде MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:26:58.566980Z",
     "start_time": "2019-09-18T11:26:58.237111Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.)\n",
    "env, n_actions, _ = get_env(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:26:59.458476Z",
     "start_time": "2019-09-18T11:26:59.453077Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск оптимального количества параллельных легковесных процессов (нитей) на конечной машине:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_jobs_experiment_data = < запуск перебора на сетке с помощью training_experiment >\n",
    "\n",
    "sns.set(font_scale=1.)\n",
    "pkl.dump(n_jobs_experiment_data, open('MountainCar-v0_n_jobs_experiment_data.pkl', 'wb'))\n",
    "clear_output(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация результатов эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "n_jobs_vals = []\n",
    "elapsed_time_vals = []\n",
    "elapsed_time_vars = []\n",
    "\n",
    "for config in n_jobs_experiment_data:\n",
    "    n_jobs_vals.append(config['n_jobs'])\n",
    "    elapsed_time_vals.append(np.mean(config['elapsed_time_list']))\n",
    "    elapsed_time_vars.append(np.std(config['elapsed_time_list']))\n",
    "\n",
    "df = pd.DataFrame({'Количество нитей': n_jobs_vals, 'Средний ETA, сек.': elapsed_time_vals,\n",
    "                   'Стандартное отклонение ETA': elapsed_time_vars})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(font_scale=1.35)\n",
    "plt.title('Производительность 5 итераций обучения на {}'.format(n_jobs_experiment_data[0]['name']),\n",
    "          fontsize=16)\n",
    "ax = sns.scatterplot(x='Количество нитей', y='Средний ETA, сек.', size='Стандартное отклонение ETA',\n",
    "                     sizes=(40, 400), data=df)\n",
    "min_val = np.min(elapsed_time_vals)\n",
    "ax.axhline(min_val, color='red', ls='--', linewidth=2, label='{:.2f} сек.'.format(min_val))\n",
    "plt.legend()\n",
    "clear_output(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследование влияния переиспользования сэмплов в процессе обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history_length_experiment_data = < запуск перебора на сетке с помощью training_experiment >\n",
    "\n",
    "sns.set(font_scale=1.)\n",
    "pkl.dump(history_length_experiment_data, open('MountainCar-v0_history_length_experiment_data.pkl', 'wb'))\n",
    "clear_output(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты текущего эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_length_experiment_results(history_length_experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск оптимального шага обучения для многослойного перцептрона (MLP) вместе c подбором количества шагов обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lrt_experiment_data = < запуск перебора на сетке с помощью training_experiment >\n",
    "\n",
    "sns.set(font_scale=1.)\n",
    "pkl.dump(lrt_experiment_data, open('MountainCar-v0_lrt_experiment_data.pkl', 'wb'))\n",
    "clear_output(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты текущего эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lrt_experiment_results(lrt_experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение агента на задаче MountainCar-v0 с использованием подобранных ранее гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sns.set(font_scale=1.)\n",
    "agent, _ = < запуск функции train_agent для обучения агента >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация действий обученного агента в зависимости скорости и положения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:55:31.581377Z",
     "start_time": "2019-09-18T11:55:31.311601Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.)\n",
    "plt.imshow(visualize_mountain_car(env, agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэмпл детерминированной стратегии по въезду на холм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sessions(agent, \"MountainCar-v0\")\n",
    "show_video(\"MountainCar-v0\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэмпл стохастической стратегии по въезду на холм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video(\"MountainCar-v0\", \"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение агента в среде LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T12:12:40.985701Z",
     "start_time": "2019-09-18T12:12:40.844112Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.)\n",
    "env, n_actions, _ = get_env(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agent, _ = < запуск функции train_agent для обучения агента >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детерминированная стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sessions(agent, 'LunarLander-v2', t_max=1000)\n",
    "show_video('LunarLander-v2', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастическая стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video('LunarLander-v2', 'sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Ещё дополнительное задание (вплоть до 5 баллов)\n",
    "\n",
    "* __2.3 дополнительное__ Обучить агента в среде с непрерывным пространством действий с помощью `MLPRegressor` ([ссылка](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)) или похожим образом.\n",
    "  * Начните с [Pendulum-v0](https://www.gymlibrary.dev/environments/classic_control/pendulum/), попробуйте получить среднее вознаграждение **как минимум -300**.\n",
    "  * Поскольку Ваш агент оценивает \"среднее\" действие, полезно добавить небольшой шум для исследования среды.\n",
    "  * Обучить агентов в [MountainCarContinuous-v0](https://mgoulao.github.io/gym-docs/environments/classic_control/mountain_car_continuous/), [LunarLanderContinuous-v2](https://www.gymlibrary.dev/environments/box2d/lunar_lander/). За достижение награды ниже порогового значения будет начислено меньше баллов. Помните, что дискретные и непрерывные среды могут отличаться не только в пространстве действий. Требования на среднее вознаграждение такие же, как и в случае сред **MountainCar** и **LunarLander** ранее.\n",
    "  * __При сдаче задания требуется перечислить в данном файле, что было сделано и чего добились__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение в Pendulum-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.)\n",
    "env, n_actions, _ = get_env('Pendulum-v0', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agent, _ = < запуск функции train_agent для обучения агента >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детерминированная стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sessions(agent, 'Pendulum-v0', t_max=1000, classification=False, epsilon=.5, agent_mul_fac=2.)\n",
    "show_video('Pendulum-v0', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастическая стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video('Pendulum-v0', 'sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.)\n",
    "env, n_actions, _ = get_env('MountainCarContinuous-v0', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agent, _ = < запуск функции train_agent для обучения агента >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детерминированная стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sessions(agent, 'MountainCarContinuous-v0', t_max=10000, classification=False, epsilon=1.0,\n",
    "                agent_mul_fac=1.0)\n",
    "show_video('MountainCarContinuous-v0', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастическая стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video('MountainCarContinuous-v0', 'sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение LunarLanderContinuous-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.)\n",
    "env, n_actions, _ = get_env('LunarLanderContinuous-v2', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "agent, _ = < запуск функции train_agent для обучения агента >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детерминированная стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sessions(agent, 'LunarLanderContinuous-v2', t_max=1000, classification=False, epsilon=1.0,\n",
    "                agent_mul_fac=1.0)\n",
    "show_video('LunarLanderContinuous-v2', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастическая стратегия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video('LunarLanderContinuous-v2', 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
